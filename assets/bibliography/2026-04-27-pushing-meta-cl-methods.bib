@article{j_hurtado_optimizing_2021,
	title = "Optimizing Reusable Knowledge for Continual Learning via Metalearning",
	abstract = {When learning tasks over time, artificial neural networks suffer from a problem known as Catastrophic Forgetting (CF). This happens when the weights of a network are overwritten during the training of a new task causing forgetting of old information. To address this issue, we propose MetA Reusable Knowledge or MARK, a new method that fosters weight reusability instead of overwriting when learning a new task. Specifically, MARK keeps a set of shared weights among tasks. We envision these shared weights as a common Knowledge Base (KB) that is not only used to learn new tasks, but also enriched with new knowledge as the model learns new tasks. Key components behind MARK are two-fold. On the one hand, a metalearning approach provides the key mechanism to incrementally enrich the KB with new knowledge and to foster weight reusability among tasks. On the other hand, a set of trainable masks provides the key mechanism to selectively choose from the KB relevant weights to solve each task. By using MARK, we achieve state of the art results in several popular benchmarks, surpassing the best performing methods in terms of average accuracy by over 10\% on the 20-Split-MiniImageNet dataset, while achieving almost zero forgetfulness using 55\% of the number of parameters. Furthermore, an ablation study provides evidence that, indeed, MARK is learning reusable knowledge that is selectively used by each task.},
	journal = "Neural Information Processing Systems",
	author = "J. Hurtado and Alain Raymond-Sáez and Álvaro Soto",
	year = {2021},
	note = {ARXIV\_ID: 2106.05390
S2ID: 632b82658b762854c60515b71e7c68be0e29fec8},
}

@article{glen_berseth_comps_2021,
	title = "CoMPS: Continual Meta Policy Search",
	abstract = {We develop a new continual meta-learning method to address challenges in sequential multi-task learning. In this setting, the agent's goal is to achieve high reward over any sequence of tasks quickly. Prior meta-reinforcement learning algorithms have demonstrated promising results in accelerating the acquisition of new tasks. However, they require access to all tasks during training. Beyond simply transferring past experience to new tasks, our goal is to devise continual reinforcement learning algorithms that learn to learn, using their experience on previous tasks to learn new tasks more quickly. We introduce a new method, continual meta-policy search (CoMPS), that removes this limitation by meta-training in an incremental fashion, over each task in a sequence, without revisiting prior tasks. CoMPS continuously repeats two subroutines: learning a new task using RL and using the experience from RL to perform completely offline meta-learning to prepare for subsequent task learning. We find that CoMPS outperforms prior continual learning and off-policy meta-reinforcement methods on several sequences of challenging continuous control tasks.},
	journal = "International Conference on Learning Representations",
	author = "Glen Berseth and Zhiwei Zhang and Grace Zhang and Chelsea Finn and S. Levine",
	year = {2021},
	note = {ARXIV\_ID: 2112.04467
S2ID: b64b3880198289fca95e54a001da3dd336502d7a},
}

@article{vinay_ramasesh_anatomy_2020,
	title = "Anatomy of Catastrophic Forgetting: Hidden Representations and Task Semantics",
	abstract = {A central challenge in developing versatile machine learning systems is catastrophic forgetting: a model trained on tasks in sequence will suffer significant performance drops on earlier tasks. Despite the ubiquity of catastrophic forgetting, there is limited understanding of the underlying process and its causes. In this paper, we address this important knowledge gap, investigating how forgetting affects representations in neural network models. Through representational analysis techniques, we find that deeper layers are disproportionately the source of forgetting. Supporting this, a study of methods to mitigate forgetting illustrates that they act to stabilize deeper layers. These insights enable the development of an analytic argument and empirical picture relating the degree of forgetting to representational similarity between tasks. Consistent with this picture, we observe maximal forgetting occurs for task sequences with intermediate similarity. We perform empirical studies on the standard split CIFAR-10 setup and also introduce a novel CIFAR-100 based task approximating realistic input distribution shift.},
	journal = "International Conference on Learning Representations",
	author = "Vinay Ramasesh and Vinay V. Ramasesh and Vinay V. Ramasesh and Ramasesh, Vinay and Ethan Dyer and Dyer, Ethan and Maithra Raghu and Raghu, Maithra",
	month = jul,
	year = {2020},
	note = {ARXIV\_ID: 2007.07400
MAG ID: 3043058418
S2ID: 1dddfe2c8c3cce6ae7c18f7ecb89fbe664057269},
}

@article{gido_m_van_de_ven_three_2019,
	title = "Three scenarios for continual learning",
	url = {https://arxiv.org/abs/1904.07734},
	abstract = {Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning difficult for machine learning. In recent years, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more structured comparisons, we describe three continual learning scenarios based on whether at test time task identity is provided and--in case it is not--whether it must be inferred. Any sequence of well-defined tasks can be performed according to each scenario. Using the split and permuted MNIST task protocols, for each scenario we carry out an extensive comparison of recently proposed continual learning methods. We demonstrate substantial differences between the three scenarios in terms of difficulty and in terms of how efficient different methods are. In particular, when task identity must be inferred (i.e., class incremental learning), we find that regularization-based approaches (e.g., elastic weight consolidation) fail and that replaying representations of previous experiences seems required for solving this scenario.},
	journal = "arXiv: Learning",
	author = "Gido M. van de Ven and van de Ven, Gido M. and Andreas S. Tolias and Tolias, Andreas S.",
	month = apr,
	year = {2019},
	note = {ARXIV\_ID: 1904.07734
MAG ID: 2939137134
S2ID: 23f425d6cb57938ceaa98fce8133a6924c2f953b},
}

@article{raia_hadsell_embracing_2020,
	title = "Embracing Change: Continual Learning in Deep Neural Networks",
	volume = {24},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661320302199},
	doi = {10.1016/j.tics.2020.09.004},
	abstract = {Artificial intelligence research has seen enormous progress over the past few decades, but it predominantly relies on fixed datasets and stationary environments. Continual learning is an increasingly relevant area of study that asks how artificial systems might learn sequentially, as biological systems do, from a continuous stream of correlated data. In the present review, we relate continual learning to the learning dynamics of neural networks, highlighting the potential it has to considerably improve data efficiency. We further consider the many new biologically inspired approaches that have emerged in recent years, focusing on those that utilize regularization, modularity, memory, and meta-learning, and highlight some of the most promising and impactful directions.},
	number = {12},
	journal = "Trends in Cognitive Sciences",
	author = "Raia Hadsell and Dushyant Rao and Andrei Rusu and Razvan Pascanu",
	year = {2020},
	doi = {10.1016/j.tics.2020.09.004},
	pmid = {33158755},
	note = {MAG ID: 3097816393
S2ID: 34b6871b40d3389f1d5c2a89fc75664d8619490c},
	pages = {1028--1040},
}

@article{shibhansh_dohare_loss_2024,
	title = "Loss of plasticity in deep continual learning",
	doi = {10.1038/s41586-024-07711-7},
	abstract = {Artificial neural networks, deep-learning methods and the backpropagation algorithm1 form the foundation of modern machine learning and artificial intelligence. These methods are almost always used in two phases, one in which the weights of the network are updated and one in which the weights are held constant while the network is used or evaluated. This contrasts with natural learning and many applications, which require continual learning. It has been unclear whether or not deep learning methods work in continual learning settings. Here we show that they do not—that standard deep-learning methods gradually lose plasticity in continual-learning settings until they learn no better than a shallow network. We show such loss of plasticity using the classic ImageNet dataset and reinforcement-learning problems across a wide range of variations in the network and the learning algorithm. Plasticity is maintained indefinitely only by algorithms that continually inject diversity into the network, such as our continual backpropagation algorithm, a variation of backpropagation in which a small fraction of less-used units are continually and randomly reinitialized. Our results indicate that methods based on gradient descent are not enough—that sustained deep learning requires a random, non-gradient component to maintain variability and plasticity.},
	journal = "The Naturalist",
	author = "Shibhansh Dohare and J. F. Hernandez-Garcia and Qingfeng Lan and Parash Rahman and A. Mahmood and R. Sutton",
	year = {2024},
	doi = {10.1038/s41586-024-07711-7},
	pmcid = {11338828},
	pmid = {39169245},
	note = {S2ID: 2a0a49d69b854b66e22a9176d77984cca112d9d3},
}

@article{jaehyeon_son_when_2023,
	title = "When Meta-Learning Meets Online and Continual Learning: A Survey",
	url = {https://ieeexplore.ieee.org/document/10684017},
	doi = {10.1109/tpami.2024.3463709},
	abstract = {Over the past decade, deep neural networks have demonstrated significant success using the training scheme that involves mini-batch stochastic gradient descent on extensive datasets. Expanding upon this accomplishment, there has been a surge in research exploring the application of neural networks in other learning scenarios. One notable framework that has garnered significant attention is meta-learning. Often described as “learning to learn,” meta-learning is a data-driven approach to optimize the learning algorithm. Other branches of interest are continual learning and online learning, both of which involve incrementally updating a model with streaming data. While these frameworks were initially developed independently, recent works have started investigating their combinations, proposing novel problem settings and learning algorithms. However, due to the elevated complexity and lack of unified terminology, discerning differences between the learning frameworks can be challenging even for experienced researchers. To facilitate a clear understanding, this paper provides a comprehensive survey that organizes various problem settings using consistent terminology and formal descriptions. By offering an overview of these learning paradigms, our work aims to foster further advancements in this promising area of research.},
	journal = "IEEE Transactions on Pattern Analysis and Machine Intelligence",
	author = "Jaehyeon Son and Soochan Lee and Gunhee Kim",
	year = {2023},
	doi = {10.1109/tpami.2024.3463709},
	pmid = {39292581},
	note = {ARXIV\_ID: 2311.05241
S2ID: 04396f17e2bdc848300b8670104895b0b3fee84f},
}

@article{eugene_lee_few-shot_2021,
	title = "Few-Shot and Continual Learning with Attentive Independent Mechanisms",
	url = {https://openaccess.thecvf.com/content/ICCV2021/papers/Lee_Few-Shot_and_Continual_Learning_With_Attentive_Independent_Mechanisms_ICCV_2021_paper.pdf},
	doi = {10.1109/iccv48922.2021.00932},
	abstract = {Deep neural networks (DNNs) are known to perform well when deployed to test distributions that shares high similarity with the training distribution. Feeding DNNs with new data sequentially that were unseen in the training distribution has two major challenges — fast adaptation to new tasks and catastrophic forgetting of old tasks. Such difficulties paved way for the on-going research on few-shot learning and continual learning. To tackle these problems, we introduce Attentive Independent Mechanisms (AIM). We incorporate the idea of learning using fast and slow weights in conjunction with the decoupling of the feature extraction and higher-order conceptual learning of a DNN. AIM is designed for higher-order conceptual learning, modeled by a mixture of experts that compete to learn independent concepts to solve a new task. AIM is a modular component that can be inserted into existing deep learning frameworks. We demonstrate its capability for few-shot learning by adding it to SIB and trained on MiniImageNet and CIFAR-FS, showing significant improvement. AIM is also applied to ANML and OML trained on Omniglot, CIFAR-100 and MiniImageNet to demonstrate its capability in continual learning. Code made publicly available at https://github.com/huang50213/AIM-Fewshot-Continual.},
	journal = "IEEE International Conference on Computer Vision",
	author = "Eugene Lee and Cheng-Han Huang and Chen-Yi Lee",
	year = {2021},
	doi = {10.1109/iccv48922.2021.00932},
	note = {ARXIV\_ID: 2107.14053
S2ID: 46cb086cbc98b80ad4094f157ec3ad3774fcbda1},
}

@article{beaulieu_learning_2020,
	title = "Learning to Continually Learn",
	url = {https://arxiv.org/abs/2002.09571},
	doi = {10.3233/faia200193},
	abstract = {Continual lifelong learning requires an agent or model to learn many sequentially ordered tasks, building on previous knowledge without catastrophically forgetting it. Much work has gone towards preventing the default tendency of machine learning models to catastrophically forget, yet virtually all such work involves manually-designed solutions to the problem. We instead advocate meta-learning a solution to catastrophic forgetting, allowing AI to learn to continually learn. Inspired by neuromodulatory processes in the brain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It differentiates through a sequential learning process to meta-learn an activation-gating function that enables context-dependent selective activation within a deep neural network. Specifically, a neuromodulatory (NM) neural network gates the forward pass of another (otherwise normal) neural network called the prediction learning network (PLN). The NM network also thus indirectly controls selective plasticity (i.e. the backward pass of) the PLN. ANML enables continual learning without catastrophic forgetting at scale: it produces state-of-the-art continual learning performance, sequentially learning as many as 600 classes (over 9,000 SGD updates).},
	journal = "European Conference on Artificial Intelligence",
	author = "Beaulieu, Shawn L. E. and Frati, Lapo and Miconi, Thomas and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff and Cheney, Nick",
	year = {2020},
	doi = {10.3233/faia200193},
	note = {ARXIV\_ID: 2002.09571
MAG ID: 3090618925
S2ID: 4bd7cc7d1fd2454956bafae1e00d2507bcbf5702},
	pages = {992--1001},
}

@article{khurram_javed_meta-learning_2019,
	title = "Meta-Learning Representations for Continual Learning",
	volume = {32},
	url = {https://papers.nips.cc/paper_files/paper/2019/hash/f4dd765c12f2ef67f98f3558c282a9cd-Abstract.html},
	abstract = {The reviews had two major concerns: lack of a benchmarking on a complex dataset, and unclear writing. To address these two major issues we: 1- Rewrote experiments section with improved terminology to make the paper more clear. Previously we were using the term Pretraining to refer to both a baseline and the meta-training stage. As the reviewers pointed out, this was confusing. We have replaced one of the usages with 'meta-training.' We have also changed evaluation to meta-testing. 2- Added mini-imagenet experiments to show that the proposed method scales to more complex datasets. Moreover, it wasn't clear if the objective we introduced improved over a maml like objective that also learned representations. We added MAML-Rep as a baseline that shows that our method -- which minimizes interference in addition to maximizing fast adaptation -- performs noticeably better. We also added the pseudo-code of the algorithms to the main paper as requested by reviewers. Moreover, we contrast our algorithm with MAML to highlight the difference between the two. We believe that this makes the current version significantly more clear to anyone who already understands the MAML objective. We have fixed various minor issues in writing and included some missing related work. (bengio2019meta, nagabandi19, al2017continuous) that we have discovered since our initial submission. Finally, we thank the reviewers and the meta-reviewer for the feedback, which allowed us to improve the work in several aspects.},
	journal = "Neural Information Processing Systems",
	author = "Khurram Javed and Martha White",
	month = jan,
	year = {2019},
	note = {ARXIV\_ID: 1905.12588
MAG ID: 2970505118
S2ID: 475d35a691580d0082ecae212a81d9a4b6be787d},
	pages = {1818--1828},
}

@inproceedings{soochan_lee_recasting_2023,
	title = "Recasting Continual Learning as Sequence Modeling",
	url = {https://arxiv.org/abs/2310.11952},
	doi = {10.48550/arxiv.2310.11952},
	abstract = {In this work, we aim to establish a strong connection between two significant bodies of machine learning research: continual learning and sequence modeling. That is, we propose to formulate continual learning as a sequence modeling problem, allowing advanced sequence models to be utilized for continual learning. Under this formulation, the continual learning process becomes the forward pass of a sequence model. By adopting the meta-continual learning (MCL) framework, we can train the sequence model at the meta-level, on multiple continual learning episodes. As a specific example of our new formulation, we demonstrate the application of Transformers and their efficient variants as MCL methods. Our experiments on seven benchmarks, covering both classification and regression, show that sequence models can be an attractive solution for general MCL.},
	booktitle = "Proceedings of the 37th International Conference on Neural Information Processing Systems",
	author = "Soochan Lee and Juhee Son and Gun-Hee Kim",
	month = oct,
	year = {2023},
	doi = {10.48550/arxiv.2310.11952},
	note = {ARXIV\_ID: 2310.11952
MAG ID: 4387800912
S2ID: 90f17a693e82e5046979b7b2300a4876f93cde64},
}

@article{lee_learning_2024,
	title = "Learning to Continually Learn with the Bayesian Principle",
	url = {https://openreview.net/forum?id=IpPnmhjw30},
	doi = {10.48550/arxiv.2405.18758},
	abstract = {In the present era of deep learning, continual learning research is mainly focused on mitigating forgetting when training a neural network with stochastic gradient descent on a non-stationary stream of data. On the other hand, in the more classical literature of statistical machine learning, many models have sequential Bayesian update rules that yield the same learning outcome as the batch training, i.e., they are completely immune to catastrophic forgetting. However, they are often overly simple to model complex real-world data. In this work, we adopt the meta-learning paradigm to combine the strong representational power of neural networks and simple statistical models' robustness to forgetting. In our novel meta-continual learning framework, continual learning takes place only in statistical models via ideal sequential Bayesian update rules, while neural networks are meta-learned to bridge the raw data and the statistical models. Since the neural networks remain fixed during continual learning, they are protected from catastrophic forgetting. This approach not only achieves significantly improved performance but also exhibits excellent scalability. Since our approach is domain-agnostic and model-agnostic, it can be applied to a wide range of problems and easily integrated with existing model architectures.},
	journal = "International Conference on Machine Learning",
	author = "Lee, Soochan and Jeon, Hyeonseong and Son, Jaehyeon and Kim, Gunhee",
	year = {2024},
	doi = {10.48550/arxiv.2405.18758},
	note = {ARXIV\_ID: 2405.18758
S2ID: 37a349a7a46a9339cb59ac02f81d3848a62d3885},
}

@article{yichen_wu_meta_2024,
	title = "Meta Continual Learning Revisited: Implicitly Enhancing Online Hessian Approximation via Variance Reduction",
	journal = "International Conference on Learning Representations",
	author = "Yichen Wu and Long-Kai Huang and Renzhen Wang and Deyu Meng and Ying Wei",
	year = {2024},
	note = {S2ID: 111478dfeba28a12cb8398893eead2de62087b85},
}

@article{mohammadamin_banayeeanzade_generative_2021,
	title = "Generative vs. Discriminative: Rethinking The Meta-Continual Learning",
	url = {https://proceedings.neurips.cc/paper/2021/hash/b4e267d84075f66ebd967d95331fcc03-Abstract.html},
	journal = "Neural Information Processing Systems",
	author = "Mohammadamin Banayeeanzade and Rasoul Mirzaiezadeh and Hosein Hasani and Mahdieh Soleymani",
	year = {2021},
	note = {S2ID: dfd4135cc81c6dfe5ccc4d6d54b4652fa3dd831f},
}

@article{timothy_m_hospedales_meta-learning_2020,
	title = "Meta-Learning in Neural Networks: A Survey",
	volume = {44},
	url = {Meta-Learning in Neural Networks: A Survey},
	doi = {10.1109/tpami.2021.3079209},
	abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where a given task is solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many of the conventional challenges of deep learning, including data and computation bottlenecks, as well as the fundamental issue of generalization. In this survey we describe the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning, multi-task learning, and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning including few-shot learning, reinforcement learning and architecture search. Finally, we discuss outstanding challenges and promising areas for future research.},
	journal = "IEEE Transactions on Pattern Analysis and Machine Intelligence",
	author = "Timothy M. Hospedales and Antreas Antoniou and Paul Micaelli and Amos Storkey",
	month = apr,
	year = {2020},
	doi = {10.1109/tpami.2021.3079209},
	pmid = {33974543},
	note = {ARXIV\_ID: 2004.05439
MAG ID: 3015606043
S2ID: 020bb2ba5f3923858cd6882ba5c5a44ea8041ab6},
}

@misc{singh_class-incremental_2023,
	title = "Class-Incremental Continual Learning for General Purpose Healthcare Models",
	url = {http://arxiv.org/abs/2311.04301},
	doi = {10.48550/arXiv.2311.04301},
	abstract = {Healthcare clinics regularly encounter dynamic data that changes due to variations in patient populations, treatment policies, medical devices, and emerging disease patterns. Deep learning models can suffer from catastrophic forgetting when fine-tuned in such scenarios, causing poor performance on previously learned tasks. Continual learning allows learning on new tasks without performance drop on previous tasks. In this work, we investigate the performance of continual learning models on four different medical imaging scenarios involving ten classification datasets from diverse modalities, clinical specialties, and hospitals. We implement various continual learning approaches and evaluate their performance in these scenarios. Our results demonstrate that a single model can sequentially learn new tasks from different specialties and achieve comparable performance to naive methods. These findings indicate the feasibility of recycling or sharing models across the same or different medical specialties, offering another step towards the development of general-purpose medical imaging AI that can be shared across institutions.},
	urldate = {2025-10-23},
	publisher = {arXiv},
	author = "Singh, Amritpal and Gurbuz, Mustafa Burak and Gantha, Shiva Souhith and Jasti, Prahlad",
	month = nov,
	year = {2023},
	note = {arXiv:2311.04301 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: 4 pages, 1 Figure. Accepted in NeurIPS 2023 (Medical Imaging meets NeurIPS Workshop)},
	file = {Preprint PDF:/Users/batsiziki/Zotero/storage/ABCWYSW9/Singh et al. - 2023 - Class-Incremental Continual Learning for General Purpose Healthcare Models.pdf:application/pdf;Snapshot:/Users/batsiziki/Zotero/storage/RTDEG9GR/2311.html:text/html},
}

@inproceedings{harun_how_2023,
	title = "How Efficient Are Today’s Continual Learning Algorithms?",
	url = {https://ieeexplore.ieee.org/document/10208398},
	doi = {10.1109/CVPRW59228.2023.00241},
	abstract = {Supervised Continual learning involves updating a deep neural network (DNN) from an ever-growing stream of labeled data. While most work has focused on overcoming catastrophic forgetting, one of the major motivations behind continual learning is being able to efficiently update a network with new information, rather than retraining from scratch on the training dataset as it grows over time. Despite recent continual learning methods largely solving the catastrophic forgetting problem, there has been little attention paid to the efficiency of these algorithms. Here, we study recent methods for incremental class learning and illustrate that many are highly inefficient in terms of compute, memory, and storage. Some methods even require more compute than training from scratch! We argue that for continual learning to have real-world applicability, the research community cannot ignore the resources used by these algorithms. There is more to continual learning than mitigating catastrophic forgetting.},
	urldate = {2025-10-23},
	booktitle = "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
	author = "Harun, Md Yousuf and Gallardo, Jhair and Hayes, Tyler L. and Kanan, Christopher",
	month = jun,
	year = {2023},
	note = {ISSN: 2160-7516},
	keywords = {Training, Artificial neural networks, Computer vision, Conferences, Industries, Learning systems, Memory management},
	pages = {2431--2436},
	file = {Snapshot:/Users/batsiziki/Zotero/storage/PS8HBDGQ/10208398.html:text/html;Submitted Version:/Users/batsiziki/Zotero/storage/LZC4CGPJ/Harun et al. - 2023 - How Efficient Are Today’s Continual Learning Algorithms.pdf:application/pdf},
}

@article{oh_discovering_2025,
	title = "Discovering state-of-the-art reinforcement learning algorithms",
	copyright = {2025 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-025-09761-x},
	doi = {10.1038/s41586-025-09761-x},
	abstract = {Humans and other animals use powerful reinforcement learning (RL) mechanisms that have been discovered by evolution over many generations of trial and error. By contrast, artificial agents typically learn using hand-crafted learning rules. Despite decades of interest, the goal of autonomously discovering powerful RL algorithms has proven elusive7-12. In this work, we show that it is possible for machines to discover a state-of-the-art RL rule that outperforms manually-designed rules. This was achieved by meta-learning from the cumulative experiences of a population of agents across a large number of complex environments. Specifically, our method discovers the RL rule by which the agent's policy and predictions are updated. In our large-scale experiments, the discovered rule surpassed all existing rules on the well-established Atari benchmark and outperformed a number of state-of-the-art RL algorithms on challenging benchmarks that it had not seen during discovery. Our findings suggest that the RL algorithms required for advanced artificial intelligence may soon be automatically discovered from the experiences of agents, rather than manually designed.},
	language = {en},
	urldate = {2025-10-24},
	journal = "Nature",
	author = "Oh, Junhyuk and Farquhar, Greg and Kemaev, Iurii and Calian, Dan A. and Hessel, Matteo and Zintgraf, Luisa and Singh, Satinder and van Hasselt, Hado and Silver, David",
	month = oct,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science},
	pages = {1--2},
}

@incollection{mccloskey_catastrophic_1989,
	title = "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem",
	volume = {24},
	shorttitle = {Catastrophic {Interference} in {Connectionist} {Networks}},
	url = {https://www.sciencedirect.com/science/article/pii/S0079742108605368},
	abstract = {Connectionist networks in which information is stored in weights on connections among simple processing units have attracted considerable interest in cognitive science. Much of the interest centers around two characteristics of these networks. First, the weights on connections between units need not be prewired by the model builder but rather may be established through training in which items to be learned are presented repeatedly to the network and the connection weights are adjusted in small increments according to a learning algorithm. Second, the networks may represent information in a distributed fashion. This chapter discusses the catastrophic interference in connectionist networks. Distributed representations established through the application of learning algorithms have several properties that are claimed to be desirable from the standpoint of modeling human cognition. These properties include content-addressable memory and so-called automatic generalization in which a network trained on a set of items responds correctly to other untrained items within the same domain. New learning may interfere catastrophically with old learning when networks are trained sequentially. The analysis of the causes of interference implies that at least some interference will occur whenever new learning may alter weights involved in representing old learning, and the simulation results demonstrate only that interference is catastrophic in some specific networks.},
	urldate = {2025-10-24},
	booktitle = "Psychology of Learning and Motivation",
	publisher = {Academic Press},
	author = "McCloskey, Michael and Cohen, Neal J.",
	editor = {Bower, Gordon H.},
	month = jan,
	year = {1989},
	doi = {10.1016/S0079-7421(08)60536-8},
	pages = {109--165},
	file = {ScienceDirect Snapshot:/Users/batsiziki/Zotero/storage/AMDKJPQX/S0079742108605368.html:text/html},
}

@article{mcclelland_why_1995,
	title = "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory",
	volume = {102},
	issn = {0033-295X},
	shorttitle = {Why there are complementary learning systems in the hippocampus and neocortex},
	url = {https://pubmed.ncbi.nlm.nih.gov/7624455/},
	doi = {10.1037/0033-295X.102.3.419},
	abstract = {Damage to the hippocampal system disrupts recent memory but leaves remote memory intact. The account presented here suggests that memories are first stored via synaptic changes in the hippocampal system, that these changes support reinstatement of recent memories in the neocortex, that neocortical synapses change a little on each reinstatement, and that remote memory is based on accumulated neocortical changes. Models that learn via changes to connections help explain this organization. These models discover the structure in ensembles of items if learning of each item is gradual and interleaved with learning about other items. This suggests that the neocortex learns slowly to discover the structure in ensembles of experiences. The hippocampal system permits rapid learning of new items without disrupting this structure, and reinstatement of new memories interleaves them with others to integrate them into structured neocortical memory systems.},
	language = {eng},
	number = {3},
	journal = "Psychological Review",
	author = "McClelland, James L. and McNaughton, Bruce L. and O'Reilly, Randall C.",
	month = jul,
	year = {1995},
	pmid = {7624455},
	keywords = {Amnesia, Retrograde, Cerebral Cortex, Hippocampus, Humans, Learning, Memory, Neural Networks, Computer},
	pages = {419--457},
}

@article{verwimp_continual_2023,
	title = "Continual Learning: Applications and the Road Forward",
	issn = {2835-8856},
	shorttitle = {Continual {Learning}},
	url = {https://openreview.net/forum?id=axBIMcGZn9},
	abstract = {Continual learning is a subfield of machine learning, which aims to allow machine learning models to continuously learn on new data, by accumulating knowledge without forgetting what was learned in the past. In this work, we take a step back, and ask: "Why should one care about continual learning in the first place?". We set the stage by examining recent continual learning papers published at four major machine learning conferences, and show that memory-constrained settings dominate the field. Then, we discuss five open problems in machine learning, and even though they might seem unrelated to continual learning at first sight, we show that continual learning will inevitably be part of their solution. These problems are model editing, personalization and specialization, on-device learning, faster (re-)training and reinforcement learning. Finally, by comparing the desiderata from these unsolved problems and the current assumptions in continual learning, we highlight and discuss four future directions for continual learning research. We hope that this work offers an interesting perspective on the future of continual learning, while displaying its potential value and the paths we have to pursue in order to make it successful. This work is the result of the many discussions the authors had at the Dagstuhl seminar on Deep Continual Learning, in March 2023.},
	language = {en},
	urldate = {2025-10-27},
	journal = "Transactions on Machine Learning Research",
	author = "Verwimp, Eli and Aljundi, Rahaf and Ben-David, Shai and Bethge, Matthias and Cossu, Andrea and Gepperth, Alexander and Hayes, Tyler L. and Hüllermeier, Eyke and Kanan, Christopher and Kudithipudi, Dhireesha and Lampert, Christoph H. and Mundt, Martin and Pascanu, Razvan and Popescu, Adrian and Tolias, Andreas S. and Weijer, Joost van de and Liu, Bing and Lomonaco, Vincenzo and Tuytelaars, Tinne and Ven, Gido M. van de",
	month = nov,
	year = {2023},
	file = {Full Text PDF:/Users/batsiziki/Zotero/storage/HLRRJL3R/Verwimp et al. - 2023 - Continual Learning Applications and the Road Forward.pdf:application/pdf},
}

@misc{guo_roomba_nodate,
	title = "A Roomba recorded a woman on the toilet. How did screenshots end up on Facebook?",
	url = {https://www.technologyreview.com/2022/12/19/1065306/roomba-irobot-robot-vacuums-artificial-intelligence-training-data-privacy/},
	abstract = {Robot vacuum companies say your images are safe, but a sprawling global supply chain for data from our devices creates risk.},
	language = {en},
	urldate = {2025-10-27},
	journal = "MIT Technology Review",
	author = "Guo, Eileen",
	file = {Snapshot:/Users/batsiziki/Zotero/storage/DG3IP5LP/roomba-irobot-robot-vacuums-artificial-intelligence-training-data-privacy.html:text/html},
}

@inproceedings{aljundi_task-free_2019,
	address = {Long Beach, CA, USA},
	title = "Task-Free Continual Learning",
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-7281-3293-8},
	url = {https://ieeexplore.ieee.org/document/8953745/},
	doi = {10.1109/CVPR.2019.01151},
	abstract = {Methods proposed in the literature towards continual deep learning typically operate in a task-based sequential learning setup. A sequence of tasks is learned, one at a time, with all data of current task available but not of previous or future tasks. Task boundaries and identities are known at all times. This setup, however, is rarely encountered in practical applications. Therefore we investigate how to transform continual learning to an online setup. We develop a system that keeps on learning over time in a streaming fashion, with data distributions gradually changing and without the notion of separate tasks. To this end, we build on the work on Memory Aware Synapses, and show how this method can be made online by providing a protocol to decide i) when to update the importance weights, ii) which data to use to update them, and iii) how to accumulate the importance weights at each update step. Experimental results show the validity of the approach in the context of two applications: (self-)supervised learning of a face recognition model by watching soap series and learning a robot to avoid collisions.},
	language = {en},
	urldate = {2025-10-30},
	booktitle = "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
	publisher = {IEEE},
	author = "Aljundi, Rahaf and Kelchtermans, Klaas and Tuytelaars, Tinne",
	month = jun,
	year = {2019},
	pages = {11246--11255},
	file = {PDF:/Users/batsiziki/Zotero/storage/R2EYR769/Aljundi et al. - 2019 - Task-Free Continual Learning.pdf:application/pdf},
}

@article{wang_comprehensive_2024,
	title = "A Comprehensive Survey of Continual Learning: Theory, Method and Application",
	volume = {46},
	issn = {1939-3539},
	shorttitle = {A {Comprehensive} {Survey} of {Continual} {Learning}},
	url = {https://ieeexplore.ieee.org/document/10444954},
	doi = {10.1109/TPAMI.2024.3367329},
	abstract = {To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance drop of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative strategies address continual learning, and how they are adapted to particular challenges in various applications. Through an in-depth discussion of promising directions, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.},
	number = {8},
	urldate = {2025-10-30},
	journal = "IEEE Transactions on Pattern Analysis and Machine Intelligence",
	author = "Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun",
	month = aug,
	year = {2024},
	keywords = {Continual learning, Surveys, Training, catastrophic forgetting, Complexity theory, incremental learning, lifelong learning, Stability analysis, Task analysis, Testing, Visualization},
	pages = {5362--5383},
	file = {Snapshot:/Users/batsiziki/Zotero/storage/LIDVLAVW/10444954.html:text/html;Submitted Version:/Users/batsiziki/Zotero/storage/XQFHKPBD/Wang et al. - 2024 - A Comprehensive Survey of Continual Learning Theory, Method and Application.pdf:application/pdf},
}

@inproceedings{kwon_lifelearner_2024,
	address = {New York, NY, USA},
	series = {{SenSys} '23},
	title = "LifeLearner: Hardware-Aware Meta Continual Learning System for Embedded Computing Platforms",
	isbn = {979-8-4007-0414-7},
	shorttitle = {{LifeLearner}},
	url = {https://dl.acm.org/doi/10.1145/3625687.3625804},
	doi = {10.1145/3625687.3625804},
	abstract = {Continual Learning (CL) allows applications such as user personalization and household robots to learn on the fly and adapt to context. This is an important feature when context, actions, and users change. However, enabling CL on resource-constrained embedded systems is challenging due to the limited labeled data, memory, and computing capacity.In this paper, we propose LifeLearner, a hardware-aware meta continual learning system that drastically optimizes system resources (lower memory, latency, energy consumption) while ensuring high accuracy. Specifically, we (1) exploit meta-learning and rehearsal strategies to explicitly cope with data scarcity issues and ensure high accuracy, (2) effectively combine lossless and lossy compression to significantly reduce the resource requirements of CL and rehearsal samples, and (3) developed hardware-aware system on embedded and IoT platforms considering the hardware characteristics.As a result, LifeLearner achieves near-optimal CL performance, falling short by only 2.8\% on accuracy compared to an Oracle baseline. With respect to the state-of-the-art (SOTA) Meta CL method, LifeLearner drastically reduces the memory footprint (by 178.7×), end-to-end latency by 80.8--94.2\%, and energy consumption by 80.9--94.2\%. In addition, we successfully deployed LifeLearner on two edge devices and a microcontroller unit, thereby enabling efficient CL on resource-constrained platforms where it would be impractical to run SOTA methods and the far-reaching deployment of adaptable CL in a ubiquitous manner. Code is available at https://github.com/theyoungkwon/LifeLearner.},
	urldate = {2025-11-02},
	booktitle = "Proceedings of the 21st ACM Conference on Embedded Networked Sensor Systems",
	publisher = {Association for Computing Machinery},
	author = "Kwon, Young D. and Chauhan, Jagmohan and Jia, Hong and Venieris, Stylianos I. and Mascolo, Cecilia",
	month = apr,
	year = {2024},
	pages = {138--151},
	file = {Full Text PDF:/Users/batsiziki/Zotero/storage/X9WGHD8R/Kwon et al. - 2024 - LifeLearner Hardware-Aware Meta Continual Learning System for Embedded Computing Platforms.pdf:application/pdf},
}

@misc{li_metaclbench_2025,
	title = "MetaCLBench: Meta Continual Learning Benchmark on Resource-Constrained Edge Devices",
	shorttitle = {{MetaCLBench}},
	url = {http://arxiv.org/abs/2504.00174},
	doi = {10.48550/arXiv.2504.00174},
	abstract = {Meta-Continual Learning (Meta-CL) has emerged as a promising approach to minimize manual labeling efforts and system resource requirements by enabling Continual Learning (CL) with limited labeled samples. However, while existing methods have shown success in image-based tasks, their effectiveness remains unexplored for sequential time-series data from sensor systems, particularly audio inputs. To address this gap, we conduct a comprehensive benchmark study evaluating six representative Meta-CL approaches using three network architectures on five datasets from both image and audio modalities. We develop MetaCLBench, an end-to-end Meta-CL benchmark framework for edge devices to evaluate system overheads and investigate trade-offs among performance, computational costs, and memory requirements across various Meta-CL methods. Our results reveal that while many Meta-CL methods enable to learn new classes for both image and audio modalities, they impose significant computational and memory costs on edge devices. Also, we find that pre-training and meta-training procedures based on source data before deployment improve Meta-CL performance. Finally, to facilitate further research, we provide practical guidelines for researchers and machine learning practitioners implementing Meta-CL on resource-constrained environments and make our benchmark framework and tools publicly available, enabling fair evaluation across both accuracy and system-level metrics.},
	urldate = {2025-11-03},
	publisher = {arXiv},
	author = "Li, Sijia and Kwon, Young D. and Lee, Lik-Hang and Hui, Pan",
	month = mar,
	year = {2025},
	note = {arXiv:2504.00174 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/batsiziki/Zotero/storage/WIYDGH43/Li et al. - 2025 - MetaCLBench Meta Continual Learning Benchmark on Resource-Constrained Edge Devices.pdf:application/pdf;Snapshot:/Users/batsiziki/Zotero/storage/E47VMISP/2504.html:text/html},
}

@incollection{vedaldi_remind_2020,
	address = {Cham},
	title = "REMIND Your Neural Network to Prevent Catastrophic Forgetting",
	volume = {12353},
	isbn = {978-3-030-58597-6 978-3-030-58598-3},
	url = {https://link.springer.com/10.1007/978-3-030-58598-3_28},
	abstract = {People learn throughout life. However, incrementally updating conventional neural networks leads to catastrophic forgetting. A common remedy is replay, which is inspired by how the brain consolidates memory. Replay involves ﬁne-tuning a network on a mixture of new and old instances. While there is neuroscientiﬁc evidence that the brain replays compressed memories, existing methods for convolutional networks replay raw images. Here, we propose REMIND, a brain-inspired approach that enables eﬃcient replay with compressed representations. REMIND is trained in an online manner, meaning it learns one example at a time, which is closer to how humans learn. Under the same constraints, REMIND outperforms other methods for incremental class learning on the ImageNet ILSVRC-2012 dataset. We probe REMIND’s robustness to data ordering schemes known to induce catastrophic forgetting. We demonstrate REMIND’s generality by pioneering online learning for Visual Question Answering (VQA)5.},
	language = {en},
	urldate = {2025-11-03},
	booktitle = "Computer Vision – ECCV 2020",
	publisher = {Springer International Publishing},
	author = "Hayes, Tyler L. and Kafle, Kushal and Shrestha, Robik and Acharya, Manoj and Kanan, Christopher",
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58598-3_28},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {466--483},
	file = {PDF:/Users/batsiziki/Zotero/storage/HDAMXENW/Hayes et al. - 2020 - REMIND Your Neural Network to Prevent Catastrophic Forgetting.pdf:application/pdf},
}

@article{jegou_product_2011,
	title = "Product Quantization for Nearest Neighbor Search",
	volume = {33},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/document/5432202},
	doi = {10.1109/TPAMI.2010.57},
	abstract = {This paper introduces a product quantization-based approach for approximate nearest neighbor search. The idea is to decompose the space into a Cartesian product of low-dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. The euclidean distance between two vectors can be efficiently estimated from their codes. An asymmetric version increases precision, as it computes the approximate distance between a vector and a code. Experimental results show that our approach searches for nearest neighbors efficiently, in particular in combination with an inverted file system. Results for SIFT and GIST image descriptors show excellent search accuracy, outperforming three state-of-the-art approaches. The scalability of our approach is validated on a data set of two billion vectors.},
	number = {1},
	urldate = {2025-11-10},
	journal = "IEEE Transactions on Pattern Analysis and Machine Intelligence",
	author = "Jégou, Herve and Douze, Matthijs and Schmid, Cordelia",
	month = jan,
	year = {2011},
	keywords = {approximate search., Electronic mail, Euclidean distance, File systems, High-dimensional indexing, Image databases, image indexing, Indexing, Nearest neighbor searches, Neural networks, Permission, Quantization, Scalability, very large databases},
	pages = {117--128},
	file = {Snapshot:/Users/batsiziki/Zotero/storage/5CYIIJND/5432202.html:text/html;Submitted Version:/Users/batsiziki/Zotero/storage/DFV6IXNR/Jégou et al. - 2011 - Product Quantization for Nearest Neighbor Search.pdf:application/pdf},
}

@article{lake_human-level_2015,
	title = "Human-level concept learning through probabilistic program induction",
	volume = {350},
	url = {https://www.science.org/doi/abs/10.1126/science.aab3050},
	doi = {10.1126/science.aab3050},
	abstract = {People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world’s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several “visual Turing tests” probing the model’s creative generalization abilities, which in many cases are indistinguishable from human behavior.},
	number = {6266},
	urldate = {2025-11-20},
	journal = "Science",
	author = "Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.",
	month = dec,
	year = {2015},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1332--1338},
}

@inproceedings{liu_casia_2011,
	address = {Beijing, China},
	title = "CASIA Online and Offline Chinese Handwriting Databases",
	isbn = {978-1-4577-1350-7},
	url = {http://ieeexplore.ieee.org/document/6065272/},
	doi = {10.1109/ICDAR.2011.17},
	abstract = {This paper introduces a pair of online and ofﬂine Chinese handwriting databases, containing samples of isolated characters and handwritten texts. The samples were produced by 1,020 writers using Anoto pen on papers for obtaining both online trajectory data and ofﬂine images. Both the online samples and ofﬂine samples are divided into six datasets, three for isolated characters (DB1.0–1.2) and three for handwritten texts (DB2.0–2.2). The (either online or ofﬂine) datasets of isolated characters contain about 3.9 million samples of 7,356 classes (7,185 Chinese characters and 171 symbols), and the datasets of handwritten texts contain about 5,090 pages and 1.35 million character samples. Each dataset is segmented and annotated at character level, and is partitioned into standard training and test subsets. The online and ofﬂine databases can be used for the research of various handwritten document analysis tasks.},
	language = {en},
	urldate = {2025-11-21},
	booktitle = "2011 International Conference on Document Analysis and Recognition",
	publisher = {IEEE},
	author = "Liu, Cheng-Lin and Yin, Fei and Wang, Da-Han and Wang, Qiu-Feng",
	month = sep,
	year = {2011},
	pages = {37--41},
	file = {PDF:/Users/batsiziki/Zotero/storage/APMLFYVG/Liu et al. - 2011 - CASIA Online and Offline Chinese Handwriting Databases.pdf:application/pdf},
}

@inproceedings{kingma_adam_2017,
	title = "Adam: A Method for Stochastic Optimization",
	shorttitle = {Adam},
	author = "Kingma, Diederik P. and Ba, Jimmy",
	year = {2015},
	booktitle = {3rd International Conference on Learning Representations (ICLR 2015)},
	url = {http://arxiv.org/abs/1412.6980},
	eprint = {1412.6980},
	eprinttype = {arXiv}
}

@misc{yu_meta-world_2021,
	title = "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning",
	shorttitle = {Meta-{World}},
	url = {http://arxiv.org/abs/1910.10897},
	doi = {10.48550/arXiv.1910.10897},
	abstract = {Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is to enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 7 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods.},
	urldate = {2025-11-24},
	publisher = {arXiv},
	author = "Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Narayan, Avnish and Shively, Hayden and Bellathur, Adithya and Hausman, Karol and Finn, Chelsea and Levine, Sergey",
	month = jun,
	year = {2021},
	note = {arXiv:1910.10897 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
	annote = {Comment: This is an update version of a manuscript that originally appeared at CoRL 2019. Videos are here: meta-world.github.io, open-sourced code are available at: https://github.com/rlworkgroup/metaworld, and the baselines can be found at https://github.com/rlworkgroup/garage},
	file = {Preprint PDF:/Users/batsiziki/Zotero/storage/DY8E3WQK/Yu et al. - 2021 - Meta-World A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning.pdf:application/pdf;Snapshot:/Users/batsiziki/Zotero/storage/L634GDTV/1910.html:text/html},
}

@misc{noauthor_meta-world_nodate,
	title = "Meta-World",
	url = {https://meta-world.github.io/},
	urldate = {2025-11-24},
	file = {Meta-World:/Users/batsiziki/Zotero/storage/4YXXC36N/meta-world.github.io.html:text/html},
}

@article{huisman_survey_2021,
	title = "A survey of deep meta-learning",
	volume = {54},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-021-10004-4},
	doi = {10.1007/s10462-021-10004-4},
	abstract = {Deep neural networks can achieve great successes when presented with large data sets and sufficient computational resources. However, their ability to learn new concepts quickly is limited. Meta-learning is one approach to address this issue, by enabling the network to learn how to learn. The field of Deep Meta-Learning advances at great speed, but lacks a unified, in-depth overview of current techniques. With this work, we aim to bridge this gap. After providing the reader with a theoretical foundation, we investigate and summarize key methods, which are categorized into (i) metric-, (ii) model-, and (iii) optimization-based techniques. In addition, we identify the main open challenges, such as performance evaluations on heterogeneous benchmarks, and reduction of the computational costs of meta-learning.},
	language = {en},
	number = {6},
	urldate = {2025-11-24},
	journal = "Artificial Intelligence Review",
	author = "Huisman, Mike and van Rijn, Jan N. and Plaat, Aske",
	month = aug,
	year = {2021},
	keywords = {Deep learning, Few-shot learning, Learning to learn, Meta-learning, Transfer learning},
	pages = {4483--4541},
	file = {Full Text PDF:/Users/batsiziki/Zotero/storage/627CA4MZ/Huisman et al. - 2021 - A survey of deep meta-learning.pdf:application/pdf},
}

@misc{efimov_similarity_2023,
	title = "Similarity Search, Part 2: Product Quantization",
	shorttitle = {Similarity {Search}, {Part} 2},
	url = {https://towardsdatascience.com/similarity-search-product-quantization-b2a1a6397701/},
	abstract = {Learn a powerful technique to effectively compress large data},
	language = {en-US},
	urldate = {2025-11-25},
	journal = "Towards Data Science",
	author = "Efimov, Vyacheslav",
	month = may,
	year = {2023},
	file = {Snapshot:/Users/batsiziki/Zotero/storage/93I44DK3/similarity-search-product-quantization-b2a1a6397701.html:text/html},
}

@misc{noauthor_product_nodate,
	title = "Product Quantization: Compressing high-dimensional vectors by 97\% {\textbar} Pinecone",
	shorttitle = {Product {Quantization}},
	url = {https://www.pinecone.io/learn/series/faiss/product-quantization/},
	abstract = {Product quantization (PQ) is a popular method for dramatically compressing high-dimensional vectors to use 97\% less memory, and for making nearest-neighbor search speeds 5.5x faster in our tests.},
	language = {en},
	urldate = {2025-11-26},
	file = {Snapshot:/Users/batsiziki/Zotero/storage/9CYZVZHS/product-quantization.html:text/html},
}

@inproceedings{verma_manifold_2019,
	title = "Manifold Mixup: Better Representations by Interpolating Hidden States",
	shorttitle = {Manifold {Mixup}},
	url = {https://proceedings.mlr.press/v97/verma19a.html},
	abstract = {Deep neural networks excel at learning the training data, but often provide incorrect and confident predictions when evaluated on slightly different test examples. This includes distribution shifts, outliers, and adversarial examples. To address these issues, we propose {\textbackslash}manifoldmixup\{\}, a simple regularizer that encourages neural networks to predict less confidently on interpolations of hidden representations. {\textbackslash}manifoldmixup\{\} leverages semantic interpolations as additional training signal, obtaining neural networks with smoother decision boundaries at multiple levels of representation. As a result, neural networks trained with {\textbackslash}manifoldmixup\{\} learn flatter class-representations, that is, with fewer directions of variance. We prove theory on why this flattening happens under ideal conditions, validate it empirically on practical situations, and connect it to the previous works on information theory and generalization. In spite of incurring no significant computation and being implemented in a few lines of code, {\textbackslash}manifoldmixup\{\} improves strong baselines in supervised learning, robustness to single-step adversarial attacks, and test log-likelihood.},
	language = {en},
	urldate = {2025-11-26},
	booktitle = "Proceedings of the 36th International Conference on Machine Learning",
	publisher = {PMLR},
	author = "Verma, Vikas and Lamb, Alex and Beckham, Christopher and Najafi, Amir and Mitliagkas, Ioannis and Lopez-Paz, David and Bengio, Yoshua",
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {6438--6447},
	file = {Full Text PDF:/Users/batsiziki/Zotero/storage/6FUXC5KS/Verma et al. - 2019 - Manifold Mixup Better Representations by Interpolating Hidden States.pdf:application/pdf},
}

@inproceedings{finn_model-agnostic_2017,
	title = "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
	url = {https://proceedings.mlr.press/v70/finn17a.html},
	abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
	language = {en},
	urldate = {2025-11-26},
	booktitle = "Proceedings of the 34th International Conference on Machine Learning",
	publisher = {PMLR},
	author = "Finn, Chelsea and Abbeel, Pieter and Levine, Sergey",
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {1126--1135},
	file = {Full Text PDF:/Users/batsiziki/Zotero/storage/XT47MFWR/Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.pdf:application/pdf;Supplementary PDF:/Users/batsiziki/Zotero/storage/6ZZK5VSM/Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.pdf:application/pdf},
}

@inproceedings{li_generative_2024,
	title = "Generative Classifiers Avoid Shortcut Solutions",
	url = {https://openreview.net/forum?id=oCUYc7BzXQ},
	abstract = {Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on features that are spuriously correlated with the label. We show that generative classifiers, which use class-conditional generative models, can avoid this issue by modeling all features, both core and spurious, instead of mainly spurious ones. These generative classifiers are simple to train, avoiding the need for specialized augmentations, strong regularization, extra hyperparameters, or knowledge of the specific spurious correlations to avoid. We find that diffusion-based and autoregressive generative classifiers achieve state-of-the-art performance on five standard image and text distribution shift benchmarks and reduce the impact of spurious correlations in realistic applications, such as medical or satellite datasets. Finally, we carefully analyze a Gaussian toy setting to understand the inductive biases of generative classifiers, as well as the data properties that determine when generative classifiers outperform discriminative ones.},
	language = {en},
	urldate = {2025-11-26},
	author = "Li, Alexander Cong and Kumar, Ananya and Pathak, Deepak",
	month = oct,
	year = {2024},
	file = {Full Text PDF:/Users/batsiziki/Zotero/storage/VKRJ7CKS/Li et al. - 2024 - Generative Classifiers Avoid Shortcut Solutions.pdf:application/pdf},
}

@article{irie_metalearning_2024,
	title = "Metalearning Continual Learning Algorithms",
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=IaUh7CSD3k},
	abstract = {General-purpose learning systems should improve themselves in open-ended fashion in ever-changing environments. Conventional learning algorithms for neural networks, however, suffer from catastrophic forgetting (CF), i.e., previously acquired skills are forgotten when a new task is learned. Instead of hand-crafting new algorithms for avoiding CF, we propose Automated Continual Learning (ACL) to train self-referential neural networks to metalearn their own in-context continual (meta)learning algorithms. ACL encodes continual learning (CL) desiderata---good performance on both old and new tasks---into its metalearning objectives. Our experiments demonstrate that ACL effectively resolves "in-context catastrophic forgetting," a problem that naive in-context learning algorithms suffer from; ACL learned algorithms outperform both hand-crafted learning algorithms and popular meta-continual learning methods on the Split-MNIST benchmark in the replay-free setting, and enables continual learning of diverse tasks consisting of multiple standard image classification datasets. We also discuss the current limitations of in-context CL by comparing ACL with state-of-the-art CL methods that leverage pre-trained models. Overall, we bring several novel perspectives into the long-standing problem of CL.},
	language = {en},
	urldate = {2025-11-26},
	journal = "Transactions on Machine Learning Research",
	author = "Irie, Kazuki and Csordás, Róbert and Schmidhuber, Jürgen",
	month = oct,
	year = {2024},
	file = {Full Text PDF:/Users/batsiziki/Zotero/storage/XLP2QADF/Irie et al. - 2024 - Metalearning Continual Learning Algorithms.pdf:application/pdf},
}

@misc{srihari_machine_nodate,
	title = "Machine Learning: Generative and Discriminative Models",
	url = {https://cedar.buffalo.edu/~srihari/CSE574/Discriminative-Generative.pdf},
	language = {en},
	author = "Srihari, Sargur N",
	file = {PDF:/Users/batsiziki/Zotero/storage/G95YGMNS/Srihari - and Discriminative Models.pdf:application/pdf},
}

@inproceedings{wong_netscore_2019,
	address = {Cham},
	title = "NetScore: Towards Universal Metrics for Large-Scale Performance Analysis of Deep Neural Networks for Practical On-Device Edge Usage",
	isbn = {978-3-030-27272-2},
	shorttitle = {{NetScore}},
	doi = {10.1007/978-3-030-27272-2_2},
	abstract = {Much of the focus in the design of deep neural networks has been on improving accuracy, leading to more powerful yet highly complex network architectures that are difficult to deploy in practical scenarios, particularly on edge devices such as mobile and other consumer devices given their high computational and memory requirements. As a result, there has been a recent interest in the design of quantitative metrics for evaluating deep neural networks that accounts for more than just model accuracy as the sole indicator of network performance. In this study, we continue the conversation towards universal metrics for evaluating the performance of deep neural networks for practical on-device edge usage. In particular, we propose a new balanced metric called NetScore, which is designed specifically to provide a quantitative assessment of the balance between accuracy, computational complexity, and network architecture complexity of a deep neural network, which is important for on-device edge operation. In what is one of the largest comparative analysis between deep neural networks in literature, the NetScore metric, the top-1 accuracy metric, and the popular information density metric were compared across a diverse set of 60 different deep convolutional neural networks for image classification on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC 2012) dataset. The evaluation results across these three metrics for this diverse set of networks are presented in this study to act as a reference guide for practitioners in the field. The proposed NetScore metric, along with the other tested metrics, are by no means perfect, but the hope is to push the conversation towards better universal metrics for evaluating deep neural networks for use in practical on-device edge scenarios to help guide practitioners in model design for such scenarios.},
	language = {en},
	booktitle = "Image Analysis and Recognition",
	publisher = {Springer International Publishing},
	author = "Wong, Alexander",
	editor = {Karray, Fakhri and Campilho, Aurélio and Yu, Alfred},
	year = {2019},
	keywords = {Deep neural networks, Edge usage, On-device, Performance analysis},
	pages = {15--26},
	url={https://link.springer.com/chapter/10.1007/978-3-030-27272-2_2},
}